{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Creditworthiness Predictor"
      ],
      "metadata": {
        "id": "xhHp8kztkonV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing libraries"
      ],
      "metadata": {
        "id": "BxLT47GZkwKy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "FhL7dTO2wKk7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset"
      ],
      "metadata": {
        "id": "kG1hc_1xk0tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/credit_dataset.csv')"
      ],
      "metadata": {
        "id": "cFhpcU9EENQd"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFirst 5 rows of dataset\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nInfo\")\n",
        "print(len(df), df.columns)\n",
        "\n",
        "print(\"\\nNull Values\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q9P_jM0EXhI",
        "outputId": "8f147f89-e1a4-4d5f-baf3-f947d4623eff"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 5 rows of dataset\n",
            "   income_yearly  debt_total  payment_history_score  credit_limit  \\\n",
            "0      2243839.0   2212461.0                   89.0      866791.0   \n",
            "1      1092486.0    760389.0                   94.0      226047.0   \n",
            "2       573637.0    118147.0                   73.0       95509.0   \n",
            "3      1762532.0   1636321.0                   40.0      634277.0   \n",
            "4      3470650.0   3917237.0                   36.0      744828.0   \n",
            "\n",
            "   credit_used  credit_utilization_ratio credit_worthiness  \n",
            "0          NaN                      0.10              good  \n",
            "1     159434.0                      0.71               bad  \n",
            "2      64490.0                      0.68              good  \n",
            "3     614385.0                      0.97               bad  \n",
            "4      39344.0                      0.05               bad  \n",
            "\n",
            "Info\n",
            "20000 Index(['income_yearly', 'debt_total', 'payment_history_score', 'credit_limit',\n",
            "       'credit_used', 'credit_utilization_ratio', 'credit_worthiness'],\n",
            "      dtype='object')\n",
            "\n",
            "Null Values\n",
            "income_yearly               1571\n",
            "debt_total                  1588\n",
            "payment_history_score       1601\n",
            "credit_limit                1654\n",
            "credit_used                 1673\n",
            "credit_utilization_ratio    1532\n",
            "credit_worthiness              0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling missing values"
      ],
      "metadata": {
        "id": "6yLRxNyVk_ZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['payment_history_score'].fillna(df['payment_history_score'].median(), inplace=True)\n",
        "df['debt_total'].fillna(df['debt_total'].median(), inplace=True)\n",
        "df['credit_limit'].fillna(df['credit_limit'].mean(), inplace=True)\n",
        "df['credit_used'].fillna(df['credit_used'].mean(), inplace=True)\n",
        "df['income_yearly'].fillna(df['income_yearly'].mean(), inplace=True)\n",
        "df.dropna(subset=['credit_utilization_ratio'], inplace=True) # -> Because this is an important column which cannot be filled with misleading values"
      ],
      "metadata": {
        "id": "IibuTN4GEvS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEaA6BPqr_Eq",
        "outputId": "b0a80399-0dc9-4ca7-a9d3-d232742a7e7f"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "income_yearly               0\n",
            "debt_total                  0\n",
            "payment_history_score       0\n",
            "credit_limit                0\n",
            "credit_used                 0\n",
            "credit_utilization_ratio    0\n",
            "credit_worthiness           0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Selecting features\n",
        "# These are the columns of information the model should learn from\n",
        "features = df[['income_yearly', 'debt_total', 'credit_limit', 'credit_used', 'credit_utilization_ratio']]\n",
        "X = features # The input\n",
        "\n",
        "#This is the ouput the model will actually predict\n",
        "y = df['credit_worthiness'] # Good or bad\n",
        "\n",
        "#Splitting the dataset for training\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# 80% training, 20% Testing\n",
        "# Train the model on 80% of our data, and separate 20% data for us to test it on"
      ],
      "metadata": {
        "id": "W_p_xsxfKVY8"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "Z35Y4EB3mxBt",
        "outputId": "8dfff94a-85fb-4e10-f1f3-e52e221941df"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        good\n",
              "1         bad\n",
              "2        good\n",
              "3         bad\n",
              "4         bad\n",
              "         ... \n",
              "19995    good\n",
              "19996    good\n",
              "19997     bad\n",
              "19998     bad\n",
              "19999     bad\n",
              "Name: credit_worthiness, Length: 18468, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>credit_worthiness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18468 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding the data"
      ],
      "metadata": {
        "id": "lrzwza_aW3QV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alphabetical  -> Numeric data"
      ],
      "metadata": {
        "id": "ce5bncn2YPvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "la = LabelEncoder()\n",
        "y_train = la.fit_transform(y_train) # Good- 1 and bad- 0\n",
        "y_test = la.transform(y_test)"
      ],
      "metadata": {
        "id": "jlA-59zXb-Gx"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test, y.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgXGwga6l29v",
        "outputId": "571e0905-5139-4324-99c4-f03d63c6ca4c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1, 0, 0, ..., 1, 1, 0]),\n",
              " 0    good\n",
              " 1     bad\n",
              " 2    good\n",
              " 3     bad\n",
              " 4     bad\n",
              " Name: credit_worthiness, dtype: object)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making the initial model"
      ],
      "metadata": {
        "id": "rw74csVnW881"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We use a algorithm called a Random Forest — which you can imagine as a bunch of tiny decision-making trees voting together.\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "# This is the part where the model learns.\n",
        "# We feed it examples:\n",
        "\n",
        "# X_train: the financial numbers\n",
        "# y_train: whether each person was credit worthy or not -> 0 or 1 -> Good or bad\n",
        "# The model studies these examples and figures out patterns.\n",
        "\n",
        "y_pred = model.predict(X_test) # Giving it new data to test it\n",
        "test_accuracy = accuracy_score(y_test, y_pred) # -> A function to get accuracy\n",
        "\n",
        "print(f\"Accuracy: {test_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdPlZmj2_KTr",
        "outputId": "be1b26b8-f868-41f9-dad2-981baf293603"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning"
      ],
      "metadata": {
        "id": "K2tiAjBPF1UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A Random Forest has a bunch of settings (hyperparameters) that influence how smart it becomes.\n",
        "# Changing these settings can massively change accuracy.\n",
        "\n",
        "# This is called hyperparameter tuning.\n",
        "\n",
        "para_grid = {\n",
        "    'n_estimators': [10, 100, 200, 300], # how many trees should the forest have\n",
        "    'max_depth': [3, 5, 10, 20, None], # how tall each tree can grow\n",
        "    'max_features': [\"sqrt\", \"log2\", None], # how many features each tree can use when splitting\n",
        "    'min_samples_leaf': [1, 2, 4], # how many data points you need to make a final “leaf” decision\n",
        "    'bootstrap': [True, False] # should the forest use bootstrapping? - whether or not should the model should sample data with replacement\n",
        "}\n",
        "\n",
        "# We create a fresh RandomForest model which is not yet trained.\n",
        "# This is the model that RandomizedSearchCV will improve by testing different settings.\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "#RandomizedSearchCV is a hyperparameter tuning technique provided by the scikit-learn library in Python\n",
        "# Instead of trying every possible combination (which can be millions), it randomly samples a handful of combinations.\n",
        "rand_search = RandomizedSearchCV(model, para_grid, cv=10, scoring=\"accuracy\", n_jobs=-1, verbose=2)\n",
        "rand_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = rand_search.best_estimator_\n",
        "print(f\"Best Model: {best_model}\")\n",
        "print(f\"Best Score: {rand_search.best_score_:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FcN8z8WevnE",
        "outputId": "5e23db76-f6fb-417e-9bcc-1081769a0c61"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
            "Best Model: RandomForestClassifier(max_depth=10, max_features=None, min_samples_leaf=2,\n",
            "                       random_state=42)\n",
            "Best Score: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting creditworthiness from user input\n"
      ],
      "metadata": {
        "id": "pdPWSfn2oXcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_user():\n",
        "  try:\n",
        "      print(\"Credit Worthiness Predictor\")\n",
        "      print(\"\\nNote: Currency is INR\")\n",
        "\n",
        "      # We collect required financial information from the user.\n",
        "      # 'float()' is used to convert the input (which is text) into a number.\n",
        "      income = float(input(\"Enter your Income (Yearly): \"))\n",
        "      debt = float(input(\"Enter your current Debt: \"))\n",
        "      credit_limit = float(input(\"Enter your credit card limit: \"))\n",
        "      credit_used = float(input(\"Enter your credit card usage: \"))\n",
        "\n",
        "      # This tells us how much of the available credit the user has used.\n",
        "      credit_utilization = credit_used / credit_limit\n",
        "\n",
        "      # Then we create a DataFrame containing the user’s entered data.\n",
        "      user_df = pd.DataFrame([{\n",
        "          'income_yearly': income,\n",
        "          'debt_total': debt,\n",
        "          'credit_limit': credit_limit,\n",
        "          'credit_used': credit_used,\n",
        "          'credit_utilization_ratio': credit_utilization\n",
        "      }])\n",
        "\n",
        "      # The trained model predicts whether the user is credit worthy (1) or not (0).\n",
        "      prediction = best_model.predict(user_df)[0]\n",
        "\n",
        "      print(\"\\nPrediction:\", \"You are credit worthy\" if prediction == 1 else \"You are not credit worthy\")\n",
        "  except ValueError:\n",
        "      print(\"Invalid input. Please enter valid numerical values.\")\n",
        "predict_user()"
      ],
      "metadata": {
        "id": "cmKI48H3PT8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b3463f4-035f-4dec-9bcb-01c3e3aa2f84"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Credit Worthiness Predictor\n",
            "\n",
            "Note: Currency is INR\n",
            "Enter your Income (Yearly): 1200000\n",
            "Enter your current Debt: 50000\n",
            "Enter your credit card limit: 200000\n",
            "Enter your credit card usage: 30000\n",
            "\n",
            "Prediction: You are credit worthy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating how good the model is"
      ],
      "metadata": {
        "id": "vK5xDOL9oa4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Precision: Out of everyone the model said deserves a credit card, how many actually deserved it?\n",
        "# Recall: Out of everyone who truly deserved a credit card, how many did the model correctly identify?\n",
        "# F1-Score: A single score that balances both precision and recall for the “deserves a credit card” class.\n",
        "# ROC–AUC: A score that measures how well the model separates people who deserve a credit card from those who don’t\n",
        "# higher value means the model is better at telling the two groups apart.\n",
        "\n",
        "print(\"\\nModel Evaluation Metrics using the initial model:\")\n",
        "\n",
        "\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
        "print(f\"ROC AUC Score: {roc_auc:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_rep)\n",
        "\n",
        "\n",
        "print(\"\\nModel Evaluation Metrics using the best model:\")\n",
        "y_pred_1 = best_model.predict(X_test)\n",
        "y_pred_proba_1 = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "accuracy_1 = accuracy_score(y_test, y_pred_1)\n",
        "classification_rep_1 = classification_report(y_test, y_pred_1)\n",
        "roc_auc_1 = roc_auc_score(y_test, y_pred_proba_1)\n",
        "\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy_1:.2f}\")\n",
        "print(f\"ROC AUC Score: {roc_auc_1:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_rep_1)"
      ],
      "metadata": {
        "id": "H5QM7hWEpEg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92365284-54a9-4c73-ab93-77a2115371ae"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Evaluation Metrics using the initial model:\n",
            "\n",
            "Accuracy: 0.75\n",
            "ROC AUC Score: 0.84\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.56      0.64      1455\n",
            "           1       0.76      0.88      0.81      2239\n",
            "\n",
            "    accuracy                           0.75      3694\n",
            "   macro avg       0.75      0.72      0.73      3694\n",
            "weighted avg       0.75      0.75      0.74      3694\n",
            "\n",
            "\n",
            "Model Evaluation Metrics using the best model:\n",
            "\n",
            "Accuracy: 0.78\n",
            "ROC AUC Score: 0.85\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.51      0.65      1455\n",
            "           1       0.75      0.97      0.84      2239\n",
            "\n",
            "    accuracy                           0.78      3694\n",
            "   macro avg       0.83      0.74      0.75      3694\n",
            "weighted avg       0.81      0.78      0.77      3694\n",
            "\n"
          ]
        }
      ]
    }
  ]
}